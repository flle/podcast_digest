{
  "enclosure_url": "https://media.transistor.fm/743c2f04/9b2a5be7.mp3",
  "episode_id": "205719bbd8a2018faded0ff3df4dd04de30313deceaf85c3a710787fed024d44",
  "feed_id": "Practical AI Podcast",
  "guid": "changelog.com/7/742",
  "link": "https://share.transistor.fm/s/743c2f04",
  "published_utc": null,
  "summary": "<p>Interpreting complicated models is a hot topic. How can we trust and manage AI models that we can’t explain? In this episode, Janis Klaise, a data scientist with Seldon, joins us to talk about model interpretation and Seldon’s new open source project called <a href=\"https://github.com/SeldonIO/alibi\">Alibi</a>. Janis also gives some of his thoughts on production ML/AI and how Seldon addresses related problems.</p><p><br /></p><p>Sponsors:</p><ul><li><a href=\"https://do.co/changelog\">DigitalOcean</a> – <strong>Check out DigitalOcean’s dedicated vCPU Droplets with dedicated vCPU threads.</strong> Get started for free with a $50 credit. Learn more at <a href=\"https://do.co/changelog\">do.co/changelog</a>. </li><li><a href=\"https://www.dataengineeringpodcast.com?utm_source=changelog&amp;utm_medium=podcast&amp;utm_campaign=practicalai\">DataEngPodcast</a> – A podcast about data engineering and modern data infrastructure. </li><li><a href=\"https://www.fastly.com/?utm_source=changelog&amp;utm_medium=podcast&amp;utm_campaign=changelog-sponsorship\">Fastly</a> – <strong>Our bandwidth partner.</strong> Fastly powers fast, secure, and scalable digital experiences. Move beyond your content delivery network to their powerful edge cloud platform. Learn more at <a href=\"https://www.fastly.com/?utm_source=changelog&amp;utm_medium=podcast&amp;utm_campaign=changelog-sponsorship\">fastly.com</a>. </li></ul><p>Featuring:</p><ul><li>Janis Klaise – <a href=\"https://github.com/jklaise\">GitHub</a>, <a href=\"https://www.linkedin.com/in/jklaise\">LinkedIn</a>, <a href=\"https://x.com/jklaise\">X</a></li><li>Chris Benson – <a href=\"https://chrisbenson.com\">Website</a>, <a href=\"https://github.com/chrisbenson\">GitHub</a>, <a href=\"https://www.linkedin.com/in/chrisbenson\">LinkedIn</a>, <a href=\"https://x.com/chrisbenson\">X</a></li><li>Daniel Whitenack – <a href=\"https://www.datadan.io/\">Website</a>, <a href=\"https://github.com/dwhitena\">GitHub</a>, <a href=\"https://x.com/dwhitena\">X</a></li></ul><p>Show Notes:</p><ul><li><a href=\"https://www.seldon.io/\">Seldon</a></li><li><a href=\"https://github.com/SeldonIO/seldon-core\">Seldon Core</a></li><li><a href=\"https://github.com/SeldonIO/alibi\">Alibi</a></li></ul><p>Books</p><ul><li><a href=\"https://www.amazon.com/dp/B01EFDEMS8\">“The Foundation Series” by Isaac Asimov</a></li><li><a href=\"https://christophm.github.io/interpretable-ml-book/\">“Interpretable Machine Learning” by Christoph Molnar</a></li></ul><p>Upcoming Events: </p><ul><li>Register for <a href=\"https://practicalai.fm/webinars\">upcoming webinars here</a>!</li></ul>",
  "title": "Model inspection and interpretation at Seldon"
}
