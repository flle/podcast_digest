{
  "enclosure_url": "https://media.transistor.fm/f54e5670/18a1bb99.mp3",
  "episode_id": "46537e61bfadfe0f986e506bf83f69783cb1d000c888926dd70969689f0b010f",
  "feed_id": "Practical AI Podcast",
  "guid": "changelog.com/7/634",
  "link": "https://share.transistor.fm/s/f54e5670",
  "published_utc": null,
  "summary": "<p><strong><em>Fully Connected</em></strong><em> – a series where Chris and Daniel keep you up to date with everything that’s happening in the AI community.</em></p><p>This week we discuss all things inference, which involves utilizing an already trained AI model and integrating it into the software stack. First, we focus on some new hardware from Amazon for inference and NVIDIA’s open sourcing of TensorRT for GPU-optimized inference. Then we talk about performing inference at the edge and in the browser with things like the recently announced ONNX JS.</p><p><br /></p><p>Sponsors:</p><ul><li><a href=\"https://do.co/changelog\">DigitalOcean</a> – DigitalOcean is simplicity at scale. Whether your business is running one virtual machine or ten thousand, DigitalOcean gets out of your way so your team can build, deploy, and scale faster and more efficiently. New accounts get $100 in credit to use in your first 60 days. </li><li><a href=\"https://www.fastly.com/?utm_source=changelog&amp;utm_medium=podcast&amp;utm_campaign=changelog-sponsorship\">Fastly</a> – <strong>Our bandwidth partner.</strong> Fastly powers fast, secure, and scalable digital experiences. Move beyond your content delivery network to their powerful edge cloud platform. Learn more at <a href=\"https://www.fastly.com/?utm_source=changelog&amp;utm_medium=podcast&amp;utm_campaign=changelog-sponsorship\">fastly.com</a>. </li><li><a href=\"https://rollbar.com/changelog\">Rollbar</a> – <strong>We catch our errors before our users do because of Rollbar.</strong> Resolve errors in minutes, and deploy your code with confidence. Learn more at <a href=\"https://rollbar.com/changelog\">rollbar.com/changelog</a>. </li><li><a href=\"https://linode.com/changelog\">Linode</a> – <strong>Our cloud server of choice.</strong> Deploy a fast, efficient, native SSD cloud server for only $5/month. Get 4 months free using the code changelog2018. Start your server - head to <a href=\"https://linode.com/changelog\">linode.com/changelog</a></li></ul><p>Featuring:</p><ul><li>Chris Benson – <a href=\"https://chrisbenson.com\">Website</a>, <a href=\"https://github.com/chrisbenson\">GitHub</a>, <a href=\"https://www.linkedin.com/in/chrisbenson\">LinkedIn</a>, <a href=\"https://x.com/chrisbenson\">X</a></li><li>Daniel Whitenack – <a href=\"https://www.datadan.io/\">Website</a>, <a href=\"https://github.com/dwhitena\">GitHub</a>, <a href=\"https://x.com/dwhitena\">X</a></li></ul><p>Show Notes:</p><p>News:</p><ul><li><a href=\"https://news.developer.nvidia.com/nvidia-tensorrt-inference-server-now-open-source/\">NVIDIA’s open sourcing of TensorRT</a></li><li>Amazon launches <a href=\"https://www.cnbc.com/2018/11/28/reuters-america-amazon-launches-machine-learning-chip-taking-on-nvidia-intel.html\">a machine learning chip</a></li><li>The recently announced <a href=\"https://github.com/Microsoft/onnxjs\">ONNX JS project</a></li><li><a href=\"https://developer.qualcomm.com/docs/snpe/overview.html\">Snapdragon Neural Processing Engine SDK</a></li></ul><p>Learning resources:</p><ul><li><a href=\"https://medium.com/@vikati/the-rise-of-the-model-servers-9395522b6c58\">Rise of the model servers</a></li><li><a href=\"https://devblogs.nvidia.com/nvidia-serves-deep-learning-inference/\">TensorRT server tutorial</a></li><li><a href=\"https://github.com/Microsoft/onnxjs\">ONNX JS</a> on GitHub</li><li><a href=\"https://js.tensorflow.org/tutorials/\">TensorFlow JS tutorials</a></li></ul><p>Upcoming Events: </p><ul><li>Register for <a href=\"https://practicalai.fm/webinars\">upcoming webinars here</a>!</li></ul>",
  "title": "So you have an AI model, now what?"
}
